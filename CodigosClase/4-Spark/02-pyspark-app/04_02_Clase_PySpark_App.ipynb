{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Desarrollo de Aplicaciones en PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Descripción de la aplicación\n",
    "##   La aplicación desarrollada será el conteo de frecuencia de palabras, desarrollado en el tutorial ‘WordCount en Spark’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preparación de los datos\n",
    "##   En este proceso, se que los datos originales se encuentran en una carpeta de la máquina local del usuario. Para este ejemplo, se crea el directorio wordcounten \n",
    "##   la carpeta actual y se crean tres archivos dentro de él."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Creación de la carpeta wordcount en la máquina local.\n",
    "##\n",
    "!mkdir -p wordcount/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing wordcount/text0.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcount/text0.txt\n",
    "Analytics is the discovery, interpretation, and communication of meaningful patterns\n",
    "in data. Especially valuable in areas rich with recorded information, analytics relies\n",
    "on the simultaneous application of statistics, computer programming and operations research\n",
    "to quantify performance.\n",
    "\n",
    "Organizations may apply analytics to business data to describe, predict, and improve business\n",
    "performance. Specifically, areas within analytics include predictive analytics, prescriptive\n",
    "analytics, enterprise decision management, descriptive analytics, cognitive analytics, Big\n",
    "Data Analytics, retail analytics, store assortment and stock-keeping unit optimization,\n",
    "marketing optimization and marketing mix modeling, web analytics, call analytics, speech\n",
    "analytics, sales force sizing and optimization, price and promotion modeling, predictive\n",
    "science, credit risk analysis, and fraud analytics. Since analytics can require extensive\n",
    "computation (see big data), the algorithms and software used for analytics harness the most\n",
    "current methods in computer science, statistics, and mathematics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing wordcount/text1.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcount/text1.txt\n",
    "The field of data analysis. Analytics often involves studying past historical data to\n",
    "research potential trends, to analyze the effects of certain decisions or events, or to\n",
    "evaluate the performance of a given tool or scenario. The goal of analytics is to improve\n",
    "the business by gaining knowledge which can be used to make improvements or changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing wordcount/text2.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcount/text2.txt\n",
    "Data analytics (DA) is the process of examining data sets in order to draw conclusions\n",
    "about the information they contain, increasingly with the aid of specialized systems\n",
    "and software. Data analytics technologies and techniques are widely used in commercial\n",
    "industries to enable organizations to make more-informed business decisions and by\n",
    "scientists and researchers to verify or disprove scientific models, theories and\n",
    "hypotheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Copia de los datos de entrada al sistema HDFS\n",
    "##   En esta aplicación se supone que los datos siempre estarán en la carpeta wordcount del directorio actual de trabajo de la máquina local. El primer paso \n",
    "##   consisten en mover los archivos de la máquina local al sistema HDFS. Por ahora, este paso se hará manualmente. Se define que la aplicación usará siempre \n",
    "##   la carpeta /tmp/wordcount/ del sistema HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Se crea la carpeta /tmp/wc en el sistema HDFS.\n",
    "##\n",
    "!hdfs dfs -mkdir /tmp/wordcount\n",
    "!hdfs dfs -mkdir /tmp/wordcount/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Copia los archvios del directorio local wordcount/\n",
    "## al directorio /tmp/wordcount/input en el hdfs\n",
    "##\n",
    "!hdfs dfs -copyFromLocal wordcount/* /tmp/wordcount/input/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "-rw-r--r--   1 root supergroup       1082 2019-11-28 02:28 /tmp/wordcount/input/text0.txt\n",
      "-rw-r--r--   1 root supergroup        349 2019-11-28 02:28 /tmp/wordcount/input/text1.txt\n",
      "-rw-r--r--   1 root supergroup        435 2019-11-28 02:28 /tmp/wordcount/input/text2.txt\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Verifica que los archivos esten copiados\n",
    "## en el hdfs\n",
    "##\n",
    "!hdfs dfs -ls /tmp/wordcount/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementación del programa en PySpark\n",
    "##   El archivo wordcount.py contiene la implementación de la aplicación. El código es el siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing wordcount.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcount.py\n",
    "\n",
    "import findspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from operator import add\n",
    "\n",
    "APP_NAME = \"wordcount-app\"\n",
    "\n",
    "findspark.init()\n",
    "conf = SparkConf().setAppName(APP_NAME)\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "## Lee los archivos de la carpeta de entrada\n",
    "text = sc.textFile(\"/tmp/wordcount/input/*.txt\")\n",
    "\n",
    "## Este es el algoritmo para el conteo de frecuencia\n",
    "words = text.flatMap(lambda x: x.split())\n",
    "wc = words.map(lambda x: (x,1))\n",
    "counts = wc.reduceByKey(add)\n",
    "\n",
    "## Escribe los resultados en la carpeta de salida.\n",
    "counts.saveAsTextFile(\"/tmp/wordcount/output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/11/28 02:29:58 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "19/11/28 02:30:00 INFO spark.SparkContext: Running Spark version 2.4.4\n",
      "19/11/28 02:30:00 INFO spark.SparkContext: Submitted application: wordcount-app\n",
      "19/11/28 02:30:00 INFO spark.SecurityManager: Changing view acls to: root\n",
      "19/11/28 02:30:00 INFO spark.SecurityManager: Changing modify acls to: root\n",
      "19/11/28 02:30:00 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "19/11/28 02:30:00 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "19/11/28 02:30:00 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
      "19/11/28 02:30:00 INFO util.Utils: Successfully started service 'sparkDriver' on port 43263.\n",
      "19/11/28 02:30:00 INFO spark.SparkEnv: Registering MapOutputTracker\n",
      "19/11/28 02:30:00 INFO spark.SparkEnv: Registering BlockManagerMaster\n",
      "19/11/28 02:30:00 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "19/11/28 02:30:00 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "19/11/28 02:30:00 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-f55eb792-97e4-45a6-90d5-c612ef38be91\n",
      "19/11/28 02:30:00 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MB\n",
      "19/11/28 02:30:00 INFO spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "19/11/28 02:30:00 INFO util.log: Logging initialized @3501ms\n",
      "19/11/28 02:30:01 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown\n",
      "19/11/28 02:30:01 INFO server.Server: Started @3593ms\n",
      "19/11/28 02:30:01 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "19/11/28 02:30:01 INFO server.AbstractConnector: Started ServerConnector@37ef51c{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}\n",
      "19/11/28 02:30:01 INFO util.Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "19/11/28 02:30:01 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3e2a6449{/jobs,null,AVAILABLE,@Spark}\n",
      "19/11/28 02:30:01 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@71f94e2a{/jobs/json,null,AVAILABLE,@Spark}\n",
      "19/11/28 02:30:01 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@331f77e9{/jobs/job,null,AVAILABLE,@Spark}\n",
      "19/11/28 02:30:01 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6cf4069a{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "19/11/28 02:30:01 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@57c78d30{/stages,null,AVAILABLE,@Spark}\n",
      "19/11/28 02:30:01 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1f2f181e{/stages/json,null,AVAILABLE,@Spark}\n",
      "19/11/28 02:30:01 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@445cbfd4{/stages/stage,null,AVAILABLE,@Spark}\n",
      "19/11/28 02:30:01 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@49d08a0{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "19/11/28 02:30:01 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@259eb5f8{/stages/pool,null,AVAILABLE,@Spark}\n",
      "19/11/28 02:30:01 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@28cb4061{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "19/11/28 02:30:01 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@630ceaef{/storage,null,AVAILABLE,@Spark}\n",
      "19/11/28 02:30:01 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@f382626{/storage/json,null,AVAILABLE,@Spark}\n",
      "19/11/28 02:30:01 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5f59634c{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "19/11/28 02:30:01 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2de3e88e{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "19/11/28 02:30:01 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@293c244f{/environment,null,AVAILABLE,@Spark}\n",
      "19/11/28 02:30:01 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@67fb2e58{/environment/json,null,AVAILABLE,@Spark}\n",
      "19/11/28 02:30:01 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@732c62e8{/executors,null,AVAILABLE,@Spark}\n",
      "19/11/28 02:30:01 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@41b9439b{/executors/json,null,AVAILABLE,@Spark}\n",
      "19/11/28 02:30:01 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@9ef19bf{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "19/11/28 02:30:01 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@37d86f34{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "19/11/28 02:30:01 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@27737005{/static,null,AVAILABLE,@Spark}\n",
      "19/11/28 02:30:01 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@52cb77a{/,null,AVAILABLE,@Spark}\n",
      "19/11/28 02:30:01 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2ddebb7a{/api,null,AVAILABLE,@Spark}\n",
      "19/11/28 02:30:01 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1f6c4fcc{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "19/11/28 02:30:01 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a7d959d{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "19/11/28 02:30:01 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://76028326a16e:4041\n",
      "19/11/28 02:30:01 INFO executor.Executor: Starting executor ID driver on host localhost\n",
      "19/11/28 02:30:01 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42761.\n",
      "19/11/28 02:30:01 INFO netty.NettyBlockTransferService: Server created on 76028326a16e:42761\n",
      "19/11/28 02:30:01 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "19/11/28 02:30:01 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 76028326a16e, 42761, None)\n",
      "19/11/28 02:30:01 INFO storage.BlockManagerMasterEndpoint: Registering block manager 76028326a16e:42761 with 366.3 MB RAM, BlockManagerId(driver, 76028326a16e, 42761, None)\n",
      "19/11/28 02:30:01 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 76028326a16e, 42761, None)\n",
      "19/11/28 02:30:01 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 76028326a16e, 42761, None)\n",
      "19/11/28 02:30:01 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4181cb4d{/metrics/json,null,AVAILABLE,@Spark}\n",
      "19/11/28 02:30:02 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 332.8 KB, free 366.0 MB)\n",
      "19/11/28 02:30:02 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.1 KB, free 365.9 MB)\n",
      "19/11/28 02:30:02 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 76028326a16e:42761 (size: 28.1 KB, free: 366.3 MB)\n",
      "19/11/28 02:30:02 INFO spark.SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
      "19/11/28 02:30:03 INFO mapred.FileInputFormat: Total input files to process : 3\n",
      "19/11/28 02:30:03 INFO Configuration.deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n",
      "19/11/28 02:30:03 INFO io.HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "19/11/28 02:30:03 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/28 02:30:03 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "19/11/28 02:30:03 INFO spark.SparkContext: Starting job: runJob at SparkHadoopWriter.scala:78\n",
      "19/11/28 02:30:03 INFO scheduler.DAGScheduler: Registering RDD 3 (reduceByKey at /datalake/wordcount.py:18)\n",
      "19/11/28 02:30:03 INFO scheduler.DAGScheduler: Got job 0 (runJob at SparkHadoopWriter.scala:78) with 4 output partitions\n",
      "19/11/28 02:30:03 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (runJob at SparkHadoopWriter.scala:78)\n",
      "19/11/28 02:30:03 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\n",
      "19/11/28 02:30:03 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 0)\n",
      "19/11/28 02:30:03 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /datalake/wordcount.py:18), which has no missing parents\n",
      "19/11/28 02:30:03 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 10.8 KB, free 365.9 MB)\n",
      "19/11/28 02:30:03 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KB, free 365.9 MB)\n",
      "19/11/28 02:30:03 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 76028326a16e:42761 (size: 7.0 KB, free: 366.3 MB)\n",
      "19/11/28 02:30:03 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161\n",
      "19/11/28 02:30:03 INFO scheduler.DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /datalake/wordcount.py:18) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
      "19/11/28 02:30:03 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 4 tasks\n",
      "19/11/28 02:30:03 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 7898 bytes)\n",
      "19/11/28 02:30:03 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 7898 bytes)\n",
      "19/11/28 02:30:03 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 7898 bytes)\n",
      "19/11/28 02:30:03 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 7898 bytes)\n",
      "19/11/28 02:30:03 INFO executor.Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
      "19/11/28 02:30:03 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "19/11/28 02:30:03 INFO executor.Executor: Running task 3.0 in stage 0.0 (TID 3)\n",
      "19/11/28 02:30:03 INFO executor.Executor: Running task 2.0 in stage 0.0 (TID 2)\n",
      "19/11/28 02:30:03 INFO rdd.HadoopRDD: Input split: hdfs://0.0.0.0:9000/tmp/wordcount/input/text0.txt:933+149\n",
      "19/11/28 02:30:03 INFO rdd.HadoopRDD: Input split: hdfs://0.0.0.0:9000/tmp/wordcount/input/text1.txt:0+349\n",
      "19/11/28 02:30:03 INFO rdd.HadoopRDD: Input split: hdfs://0.0.0.0:9000/tmp/wordcount/input/text0.txt:0+933\n",
      "19/11/28 02:30:03 INFO rdd.HadoopRDD: Input split: hdfs://0.0.0.0:9000/tmp/wordcount/input/text2.txt:0+435\n",
      "19/11/28 02:30:04 INFO python.PythonRunner: Times: total = 654, boot = 573, init = 53, finish = 28\n",
      "19/11/28 02:30:04 INFO python.PythonRunner: Times: total = 540, boot = 465, init = 66, finish = 9\n",
      "19/11/28 02:30:04 INFO python.PythonRunner: Times: total = 638, boot = 558, init = 56, finish = 24\n",
      "19/11/28 02:30:04 INFO python.PythonRunner: Times: total = 621, boot = 539, init = 74, finish = 8\n",
      "19/11/28 02:30:04 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 1678 bytes result sent to driver\n",
      "19/11/28 02:30:04 INFO executor.Executor: Finished task 3.0 in stage 0.0 (TID 3). 1678 bytes result sent to driver\n",
      "19/11/28 02:30:04 INFO executor.Executor: Finished task 2.0 in stage 0.0 (TID 2). 1678 bytes result sent to driver\n",
      "19/11/28 02:30:04 INFO executor.Executor: Finished task 1.0 in stage 0.0 (TID 1). 1721 bytes result sent to driver\n",
      "19/11/28 02:30:04 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1399 ms on localhost (executor driver) (1/4)\n",
      "19/11/28 02:30:04 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 1383 ms on localhost (executor driver) (2/4)\n",
      "19/11/28 02:30:04 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 1386 ms on localhost (executor driver) (3/4)\n",
      "19/11/28 02:30:04 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1388 ms on localhost (executor driver) (4/4)\n",
      "19/11/28 02:30:04 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "19/11/28 02:30:04 INFO python.PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 44273\n",
      "19/11/28 02:30:04 INFO scheduler.DAGScheduler: ShuffleMapStage 0 (reduceByKey at /datalake/wordcount.py:18) finished in 1.515 s\n",
      "19/11/28 02:30:04 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "19/11/28 02:30:04 INFO scheduler.DAGScheduler: running: Set()\n",
      "19/11/28 02:30:04 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 1)\n",
      "19/11/28 02:30:04 INFO scheduler.DAGScheduler: failed: Set()\n",
      "19/11/28 02:30:04 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[8] at saveAsTextFile at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "19/11/28 02:30:04 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 90.1 KB, free 365.8 MB)\n",
      "19/11/28 02:30:04 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.2 KB, free 365.8 MB)\n",
      "19/11/28 02:30:04 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 76028326a16e:42761 (size: 34.2 KB, free: 366.2 MB)\n",
      "19/11/28 02:30:04 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1161\n",
      "19/11/28 02:30:04 INFO scheduler.DAGScheduler: Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[8] at saveAsTextFile at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
      "19/11/28 02:30:04 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 4 tasks\n",
      "19/11/28 02:30:04 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 4, localhost, executor driver, partition 0, ANY, 7662 bytes)\n",
      "19/11/28 02:30:04 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 5, localhost, executor driver, partition 1, ANY, 7662 bytes)\n",
      "19/11/28 02:30:04 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 1.0 (TID 6, localhost, executor driver, partition 2, ANY, 7662 bytes)\n",
      "19/11/28 02:30:04 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 1.0 (TID 7, localhost, executor driver, partition 3, ANY, 7662 bytes)\n",
      "19/11/28 02:30:04 INFO executor.Executor: Running task 2.0 in stage 1.0 (TID 6)\n",
      "19/11/28 02:30:04 INFO executor.Executor: Running task 1.0 in stage 1.0 (TID 5)\n",
      "19/11/28 02:30:04 INFO executor.Executor: Running task 3.0 in stage 1.0 (TID 7)\n",
      "19/11/28 02:30:04 INFO executor.Executor: Running task 0.0 in stage 1.0 (TID 4)\n",
      "19/11/28 02:30:04 INFO storage.ShuffleBlockFetcherIterator: Getting 4 non-empty blocks including 4 local blocks and 0 remote blocks\n",
      "19/11/28 02:30:04 INFO storage.ShuffleBlockFetcherIterator: Getting 4 non-empty blocks including 4 local blocks and 0 remote blocks\n",
      "19/11/28 02:30:04 INFO storage.ShuffleBlockFetcherIterator: Getting 4 non-empty blocks including 4 local blocks and 0 remote blocks\n",
      "19/11/28 02:30:04 INFO storage.ShuffleBlockFetcherIterator: Getting 4 non-empty blocks including 4 local blocks and 0 remote blocks\n",
      "19/11/28 02:30:04 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms\n",
      "19/11/28 02:30:04 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms\n",
      "19/11/28 02:30:04 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms\n",
      "19/11/28 02:30:04 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 16 ms\n",
      "19/11/28 02:30:05 INFO io.HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "19/11/28 02:30:05 INFO io.HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "19/11/28 02:30:05 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/28 02:30:05 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "19/11/28 02:30:05 INFO io.HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "19/11/28 02:30:05 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/28 02:30:05 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "19/11/28 02:30:05 INFO io.HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "19/11/28 02:30:05 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/28 02:30:05 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "19/11/28 02:30:05 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/28 02:30:05 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "19/11/28 02:30:05 INFO python.PythonRunner: Times: total = 56, boot = -732, init = 788, finish = 0\n",
      "19/11/28 02:30:05 INFO python.PythonRunner: Times: total = 56, boot = -702, init = 758, finish = 0\n",
      "19/11/28 02:30:05 INFO python.PythonRunner: Times: total = 49, boot = -734, init = 783, finish = 0\n",
      "19/11/28 02:30:05 INFO python.PythonRunner: Times: total = 51, boot = -697, init = 748, finish = 0\n",
      "19/11/28 02:30:05 INFO output.FileOutputCommitter: Saved output of task 'attempt_20191128023003_0008_m_000002_0' to hdfs://0.0.0.0:9000/tmp/wordcount/output/_temporary/0/task_20191128023003_0008_m_000002\n",
      "19/11/28 02:30:05 INFO output.FileOutputCommitter: Saved output of task 'attempt_20191128023003_0008_m_000000_0' to hdfs://0.0.0.0:9000/tmp/wordcount/output/_temporary/0/task_20191128023003_0008_m_000000\n",
      "19/11/28 02:30:05 INFO output.FileOutputCommitter: Saved output of task 'attempt_20191128023003_0008_m_000001_0' to hdfs://0.0.0.0:9000/tmp/wordcount/output/_temporary/0/task_20191128023003_0008_m_000001\n",
      "19/11/28 02:30:05 INFO mapred.SparkHadoopMapRedUtil: attempt_20191128023003_0008_m_000001_0: Committed\n",
      "19/11/28 02:30:05 INFO output.FileOutputCommitter: Saved output of task 'attempt_20191128023003_0008_m_000003_0' to hdfs://0.0.0.0:9000/tmp/wordcount/output/_temporary/0/task_20191128023003_0008_m_000003\n",
      "19/11/28 02:30:05 INFO mapred.SparkHadoopMapRedUtil: attempt_20191128023003_0008_m_000000_0: Committed\n",
      "19/11/28 02:30:05 INFO mapred.SparkHadoopMapRedUtil: attempt_20191128023003_0008_m_000002_0: Committed\n",
      "19/11/28 02:30:05 INFO mapred.SparkHadoopMapRedUtil: attempt_20191128023003_0008_m_000003_0: Committed\n",
      "19/11/28 02:30:05 INFO executor.Executor: Finished task 0.0 in stage 1.0 (TID 4). 2067 bytes result sent to driver\n",
      "19/11/28 02:30:05 INFO executor.Executor: Finished task 1.0 in stage 1.0 (TID 5). 2067 bytes result sent to driver\n",
      "19/11/28 02:30:05 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 4) in 256 ms on localhost (executor driver) (1/4)\n",
      "19/11/28 02:30:05 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 5) in 255 ms on localhost (executor driver) (2/4)\n",
      "19/11/28 02:30:05 INFO executor.Executor: Finished task 2.0 in stage 1.0 (TID 6). 2024 bytes result sent to driver\n",
      "19/11/28 02:30:05 INFO executor.Executor: Finished task 3.0 in stage 1.0 (TID 7). 2024 bytes result sent to driver\n",
      "19/11/28 02:30:05 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 1.0 (TID 6) in 259 ms on localhost (executor driver) (3/4)\n",
      "19/11/28 02:30:05 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 1.0 (TID 7) in 258 ms on localhost (executor driver) (4/4)\n",
      "19/11/28 02:30:05 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "19/11/28 02:30:05 INFO scheduler.DAGScheduler: ResultStage 1 (runJob at SparkHadoopWriter.scala:78) finished in 0.286 s\n",
      "19/11/28 02:30:05 INFO scheduler.DAGScheduler: Job 0 finished: runJob at SparkHadoopWriter.scala:78, took 1.889619 s\n",
      "19/11/28 02:30:05 INFO io.SparkHadoopWriter: Job job_20191128023003_0008 committed.\n",
      "\u001b[22;0t\u001b]0;IPython: /datalake\u000719/11/28 02:30:05 INFO spark.SparkContext: Invoking stop() from shutdown hook\n",
      "19/11/28 02:30:05 INFO server.AbstractConnector: Stopped Spark@37ef51c{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}\n",
      "19/11/28 02:30:05 INFO ui.SparkUI: Stopped Spark web UI at http://76028326a16e:4041\n",
      "19/11/28 02:30:05 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "19/11/28 02:30:05 INFO memory.MemoryStore: MemoryStore cleared\n",
      "19/11/28 02:30:05 INFO storage.BlockManager: BlockManager stopped\n",
      "19/11/28 02:30:05 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\n",
      "19/11/28 02:30:05 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "19/11/28 02:30:05 INFO spark.SparkContext: Successfully stopped SparkContext\n",
      "19/11/28 02:30:05 INFO util.ShutdownHookManager: Shutdown hook called\n",
      "19/11/28 02:30:05 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-39e40ee7-de8e-4d99-a171-52c690be4333\n",
      "19/11/28 02:30:05 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-5b57f80f-9312-437e-ad4d-fa9671760635\n",
      "19/11/28 02:30:05 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-5b57f80f-9312-437e-ad4d-fa9671760635/pyspark-ec2dfa45-d187-43c6-85bb-736409180805\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## La aplicación es ejecutada usando spark-submit,\n",
    "## el cual ejecuta el programa wordcount.py en Spark\n",
    "##\n",
    "!spark-submit wordcount.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Archivos de resultados\n",
    "## La carpeta /tmp/wordcount/output contiene los resultados de la ejecución del programa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 items\n",
      "-rw-r--r--   1 root supergroup          0 2019-11-28 02:30 /tmp/wordcount/output/_SUCCESS\n",
      "-rw-r--r--   1 root supergroup        778 2019-11-28 02:30 /tmp/wordcount/output/part-00000\n",
      "-rw-r--r--   1 root supergroup        562 2019-11-28 02:30 /tmp/wordcount/output/part-00001\n",
      "-rw-r--r--   1 root supergroup        510 2019-11-28 02:30 /tmp/wordcount/output/part-00002\n",
      "-rw-r--r--   1 root supergroup        594 2019-11-28 02:30 /tmp/wordcount/output/part-00003\n"
     ]
    }
   ],
   "source": [
    "## Archivos con los resultados. Note que se\n",
    "## generan varios archivos de resultados.\n",
    "!hdfs dfs -ls /tmp/wordcount/output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## El archivo output/_SUCCESS es un archivo vacio que indica que el programa fue ejecutado correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('interpretation,', 1)\n",
      "('of', 8)\n",
      "('in', 5)\n",
      "('data.', 1)\n",
      "('Especially', 1)\n",
      "('analytics', 8)\n",
      "('simultaneous', 1)\n",
      "('operations', 1)\n",
      "('research', 2)\n",
      "('quantify', 1)\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /tmp/wordcount/output/part-00000 | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Archivo de comandos del sistema operativo\n",
    "##   Finalmente, se crea un script que copie los archivos y ejecute la aplicación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing wordcountapp\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcountapp\n",
    "#! /bin/bash\n",
    "\n",
    "## borra la carpeta input si existe\n",
    "!hdfs dfs -rm -r -f /tmp/wordcount/input\n",
    "\n",
    "## crea la carpeta\n",
    "!hdfs dfs -mkdir /tmp/wordcount/input\n",
    "\n",
    "## copia los archivos de entrada de la\n",
    "## maquina local al sistema hdfs\n",
    "!hdfs dfs -copyFromLocal wordcount/* /tmp/wordcount/input/\n",
    "\n",
    "## ejecuta la aplicación de spark\n",
    "spark-submit wordcount.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Esta aplicación sería ejecutada en Terminal con el siguiente comando: $ bash wordcountapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Limpieza de las carpetas de trabajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /tmp/wordcount\n"
     ]
    }
   ],
   "source": [
    "!rm wordcountapp\n",
    "!rm -rf wordcount\n",
    "!hdfs dfs -rm -r -f /tmp/wordcount/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
